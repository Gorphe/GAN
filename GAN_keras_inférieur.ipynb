{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN keras inf√©rieur.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMR+c2HMLDULmp/OGfznrXH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filsto/GAN/blob/main/GAN_keras_inf%C3%A9rieur.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO3nFYoboysg"
      },
      "outputs": [],
      "source": [
        "from Tensorflow.keras import layers\n",
        "\n",
        "# create the discriminator\n",
        "discriminator = keras.Sequential([\n",
        "                                  keras.Input(shape=(28,28,1)),\n",
        "                                  layers.Conv2D(61,(3,3), strides=(2,2), padding='same'),\n",
        "                                  layers.LeakyReLU(alpha=0.2),\n",
        "                                  layers.Conv2D(128,(3,3), strides=(2,2), padding='same'),\n",
        "                                  layers.LeakyReLU(alpha=0.2),\n",
        "                                  layers.GlobalMaxPooling2D(),\n",
        "                                  layers.Dense(1)\n",
        "], name='discriminator')\n",
        "\n",
        "# create the generator\n",
        "latent_dim = 128\n",
        "generator = keras.Sequential([\n",
        "                              keras.Input(shape=(latent_dim,)),\n",
        "                              #we want to generate 128 coefficients to reshape into a 7x7x128 map\n",
        "                              layers.Dense(7*7*128),\n",
        "                              layers.LeakyReLU(alpha=0.2),\n",
        "                              layers.Reshape((7,7,128)),\n",
        "                              layers.Conv2DTranspose(128,(4,4),strides=(2,2), padding='same'),\n",
        "                              layers.LeakyReLU(alpha=0.2),\n",
        "                              layers.Conv2DTranspose(128,(4,4),strides=(2,2), padding='same'),\n",
        "                              layers.LeakyReLU(alpha=0.2),\n",
        "                              layers.Conv2D(1,(7,7), padding='same', activation='sigmoid'),\n",
        "], name='generator')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(keras.Model):\n",
        "  \n",
        "  def __init__(self, discriminator, generator, latent_dim):\n",
        "    super(GAN, self).__init__()\n",
        "    self.discriminator = discriminator\n",
        "    self.generator = generator\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "  def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "  def train_step(self, real_images):\n",
        "        if isinstance(real_images, tuple):\n",
        "            real_images = real_images[0]\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "                           \n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "            \n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
      ],
      "metadata": {
        "id": "PKwc4FS0qsJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_OWiuIiMqr-N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}