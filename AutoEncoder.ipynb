{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoEncoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCZQsFQv4K5stgM2JEuA/Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filsto/GAN/blob/main/AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tXSgPkzfCXkx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "print(\"size of:\")\n",
        "print(\"- training set:\\t\\t{}\".format(len(mnist.train.labels)))\n",
        "print(\"- test set:\\t\\t{}\".format(len(mnist.test.labels)))\n",
        "print(\"- validation set:\\t\\t{}\".format(len(mnist.validation.labels)))"
      ],
      "metadata": {
        "id": "NOBxXjNUCk8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "Uks1WvT7najv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyper parameters\n",
        "logs_path = \"./logs/noiseRemoval\"\n",
        "\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "batch_size = 100\n",
        "display_freq = 100\n",
        "\n",
        "#network parameters\n",
        "img_h = img_w = 28\n",
        "img_size_flat = img_h * img_w\n",
        "h1 = 100\n",
        "noise_level = 0.6"
      ],
      "metadata": {
        "id": "QFc_7EboDn1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph"
      ],
      "metadata": {
        "id": "qyRM6sTJnWym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#weight and bias wrappers\n",
        "def weight_variable(name, shape):\n",
        "  \"\"\"\n",
        "  create a weight variable with appropriate initialization\n",
        "  :param name: weight name\n",
        "  :para shape: weight shape\n",
        "  :return: intialized weight variable\n",
        "  \"\"\"\n",
        "\n",
        "  initer = tf.truncated_normal_initializer(stddev=0.01)\n",
        "  return tf.get_variable('W_' +name, dtype=tf.float32, shape=shape, initializer=initer)\n",
        "\n",
        "def bias_variable(name, shape):\n",
        "  \"\"\"\n",
        "  create a bias variable with appropriate initialization\n",
        "  :param name: bias variable name\n",
        "  :param shape: bias variable shape\n",
        "  :return: intialized bias variable\n",
        "  \"\"\"\n",
        "\n",
        "  initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
        "  return tf.get_variable('b' +name, dtype=tf.float32, initializer=initial)\n",
        "\n",
        "def fc_layer(x, num_units, name, use_relu=True):\n",
        "  \"\"\"\n",
        "  create a fully connected layer\n",
        "  :param x: input from previous layer\n",
        "  :param num_units: nb of hidden units in the fully-connected layer\n",
        "  :param name: layer name\n",
        "  :param use_relu: boolean to add ReLU non-linearity (or not)\n",
        "  :return: the output array\n",
        "  \"\"\"\n",
        "\n",
        "  with tf.variable_scope(name):\n",
        "    in_dim = x.get_shape()[1]\n",
        "    W = weight_variable(name, shape=[in_dim, num_units])\n",
        "    tf.summary.histogram('W', W)\n",
        "    b = bias_variable(name, [num_units])\n",
        "    tf.summary.histogram('b',b)\n",
        "    layer = tf.matmul(x, W)\n",
        "    layer +=b\n",
        "    if use_relu:\n",
        "      layer = tf.nn.relu(layer)\n",
        "      return layer\n"
      ],
      "metadata": {
        "id": "sLxzyoutFB3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create graphs\n",
        "#placeholders for inputs(x), outputs(y)\n",
        "with tf.variable.scope('Input'):\n",
        "  x_original = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='X_original')\n",
        "  x_noisy = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='X_noisy')\n",
        "\n",
        "fc1 = fc_layer(x_noisy, h1, 'Hidden layer', use_relu=True)\n",
        "out = fc_layer(fc1, img_size_flat, 'Output_layer', use_relu=False)\n",
        "\n",
        "#define the loss function, optimizer, and accuracy\n",
        "with tf.variable_scope('Train'):\n",
        "  with tf.variable_scope('Loss'):\n",
        "    loss = tf.reduce_mean(tf.losses.mean_squared_error(x_original, out), name='loss')\n",
        "    tf.summary.scalar('loss', loss)\n",
        "  with tf.variable_scope('Optimizer'):\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam-op').minimze(loss)\n",
        "\n",
        "#initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#add 5 images from original, noisy and reconstructed samples to summaries\n",
        "tf.summary.image('original', tf.reshape(x_original,(-1, img_w, img_h, 1)), max_outputs=5)\n",
        "tf.summary.image('noisy', tf.reshape(x_noisy, (-1, img_w, img_h, 1)), max_outputs=5)\n",
        "tf.summary.image('reconstructed', tf.reshape(out, (-1, img_w, img_h, 1)), max_outputs=5)\n",
        "\n",
        "#merge all the summaries\n",
        "merged = tf.summary.merge_all()"
      ],
      "metadata": {
        "id": "6R4V2YnIcmOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "3FvF7LL3nUEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#launch the graph (session)\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(init)\n",
        "\n",
        "train_writer = tf.summary.FileWriter(logs_path, sess.graph)\n",
        "num_tr_iter = int(mnist.train.num_examples / batch_size)\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('Training epoch: {}'.format(epoch +1))\n",
        "  for iteration in range(num_tr_iter):\n",
        "    batch_x, _ = mnist.train.next_batch(batch_size)\n",
        "    batch_x_noisy = batch_x + noise_level * np.random.normal(loc=0.0, scale=1.0, size=batch_x.shape)\n",
        "    global_step +=1\n",
        "    \n",
        "    #run optimizatyion op (backprop)\n",
        "    feed_dict_batch = {x_original: batch_x, x_noisy :batch_x_noisy}\n",
        "    _, summary_tr = sess.run([optimizer, merged], feed_dict=feed_dict_batch)\n",
        "    train_writer.add_summary(summary_tr, global_step)\n",
        "\n",
        "    if iteration % display_freq == 0:\n",
        "      #calculate and display the batch loss and accuracy\n",
        "      loss_batch = sess.run(loss, feed_dict = feed_dict_batch)\n",
        "      print('iter {0:3d}\\t Reconstruction loss={1:3f}'.format(iteration, loss_batch))\n",
        "\n",
        "  #run validation after every epoch\n",
        "  x_valid_original = mnist.validation.images\n",
        "  x_valid_noisy = x_valid_original + noise_level * np.random.normal(loc=0.0, scale=1.0, size=x_valid_original.shape)\n",
        "\n",
        "  feed_dict_valid = {x_original: x_valid_original, x_noisy: x_valid_noisy}\n",
        "  loss_valid = sess.run(loss, feed_dict = feed_dict_valid)\n",
        "  print('______________________________________')\n",
        "  print('Epoch: {0}, validation loss: {1:.3f}'.format(epoch +1, loss_valid))\n",
        "  print('______________________________________')"
      ],
      "metadata": {
        "id": "_oSHNGyYnSvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "gGDr21nAq1yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(original_images, noisy_images, reconstructed_images):\n",
        "  \"\"\"\n",
        "  create figure of original and reconstructed image.\n",
        "  :param original_image: original images to be plotted, (?, img_h*img_w)\n",
        "  :param noisy_image: noisy images to be plotted, (?, img_h*img_w)\n",
        "  :param reconstructed_image: reconstructed images to be plotted, (?, img_h*img_w)\n",
        "  \"\"\"\n",
        "\n",
        "  num_images = original_images.shape[0]\n",
        "  fig.axes = plt.subplots(num_images, 3, figsize=(9,9))\n",
        "  fig.subplots_adjust(hspace=.1, wspace=0)\n",
        "\n",
        "  img_h = img_w = np.sqrt(original_images.shape[-1]).astype(int)\n",
        "  for i, ax in enumerates(axes):\n",
        "    #plot image\n",
        "    ax[0].imshow(original_images[i].reshape((img_h, img_w)),cmap='gray')\n",
        "    ax[1].imshow(noisy_images[i].reshape((img_h, img_w)),cmap='gray')\n",
        "    ax[2].imshow(reconstructed_images[i].reshape((img_h, img_w)),cmap='gray')\n",
        "\n",
        "    #remove ticks from the plot\n",
        "    for sub_ax in ax:\n",
        "      sub_ax.set_xticks([])\n",
        "      sub_ax.set_yticks([])\n",
        "\n",
        "  for ax, col in zip(axes[0], ['Original Image','Noisy Image','reconstructed Image']):\n",
        "    ax.set_title(col)\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ccjxexL-q3cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test the network after training\n",
        "\n",
        "#make a noisy image\n",
        "test_samples = 5\n",
        "x_test = mnist.test.images[:test_samples]\n",
        "x_test_noisy = x_test + noise_level *np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "\n",
        "#reconstruct a clean image from noisy image\n",
        "x_reconstruct = sess.run(out, feed_dict={x_noisy:x_test_noisy})\n",
        "\n",
        "#calculate the loss between reconstructed image and original image\n",
        "loss_test = sess.run(loss, feed_dict={x_original:x_test, x_noisy:x_test_noisy})\n",
        "print('------------------------------------------------')\n",
        "print('test loss of original image compared to reconstructed image :{0:.3f}'.format(loss_test))\n",
        "print('------------------------------------------------')\n",
        "\n",
        "#plot original image, noisy image, and reconstructed image\n",
        "plot_images(x_test, x_test_noisy, x_reconstruct)"
      ],
      "metadata": {
        "id": "v9Rebr6WtBjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# close the session after being done with testing\n",
        "sess.close()"
      ],
      "metadata": {
        "id": "Icah6DLduhC-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}